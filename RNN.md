## RNN

一个完全连接的循环网络是任何非线性动力系统的近似器。

### 循环神经网络几种应用模式

#### 1序列到类别模式

 比如文本分类问题。

#### 2同步的序列到序列模式

比如序列标注任务，词性标注。

#### 3异步的序列到序列模式

也称为**编码器-解码器**模型，输入序列和输出序列不需要有严格的对应关系，比如机器翻译。
### 参数学习

随机时间反向传播（BPTT）和实时循环学习（RTRL）。

两种算法比较，在循环神经网络中，一般网络的输出维度远低于输入的维度，因此BPTT算法的计算量会更小一些，但是BPTT算法需要保存所有时刻的中间梯度，空间复杂度比较高。RTRL算法不需要梯度回传，因此非常适合用于需要在线学习无限序列的任务中。

### 长程依赖问题
当输入序列比较长时，会存在梯度爆炸和消失问题。

循环神经网络在学习过程的主要问题是由于梯度消失或者爆炸的问题,很难建模长时间间隔的状态之间的依赖关系。

梯度爆炸,通过权重衰减或者是梯度阶段来避免。

梯度消失,除了使用一些优化技巧外,使用改变模型等方法,依旧还是会存在两个问题1梯度爆炸的问题和2记忆容量的问题.为了解决这两个问题,可以通过引入门控机制来进一步模型。

### 基于门控的循环神经网络
长短期记忆网络和门控循环单元网络。

#### 1长短期记忆网络LSTM

f 遗忘门，i 输入门，o 输出门。

![image-20201120195514535](C:\Users\shanj\AppData\Roaming\Typora\typora-user-images\image-20201120195514535.png)

#### 2门控循环单元网络GRU

![image-20201120195525698](C:\Users\shanj\AppData\Roaming\Typora\typora-user-images\image-20201120195525698.png)

z更新门，r重置门。

### 深层循环神经网络

从横向和纵向来看循环神经网络的深度，横向深，纵向浅。

增加循环神经网络的深度主要是增加纵向的深度，比如增加x->h之间的深度以及h->y之间的深度。

#### 堆叠循环神经网络SRNN

常见的作法是将多个循环网络堆叠起来。

![image-20201120200630098](C:\Users\shanj\AppData\Roaming\Typora\typora-user-images\image-20201120200630098.png)

#### 双向循环神经网络

双向循环神经网络由两层循环神经网络组成，他们的输入相同，只是信息传递的方向不同。

![image-20201120201119367](C:\Users\shanj\AppData\Roaming\Typora\typora-user-images\image-20201120201119367.png)

### 扩展到图结构

#### 递归神经网络

递归神经网络主要用来建模自然语言句子的语义。

#### 图神经网络

